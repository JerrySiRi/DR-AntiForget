<h1 align="center"><img src="assets/icon/opencompass.png" alt="OpenCompass" height="50" style="vertical-align:middle;" />&nbsp;SciEval ToolKit</h1>

<p align="center"><strong>
A unified evaluation toolkit and leaderboard for rigorously assessing the scientific intelligence of large language and visionâ€“language models across the full research workflow.
</strong></p>

<hr style="width:100%;margin:16px 0;border:0;border-top:0.1px solid #d0d7de;" />

<div align="center">

[![Tutorial](https://img.shields.io/badge/Tutorial-SciEval-b8dcff?style=for-the-badge&logo=google-chrome&logoColor=white)](https://scievalkit-docs.readthedocs.io/en/latest)&#160;
[![Leaderboard](https://img.shields.io/badge/LEADERBOARD-Scieval-f6e58d?style=for-the-badge&logo=huggingface)](https://opencompass.org.cn/Intern-Discovery-Eval/rank)&#160;
[![Report](https://img.shields.io/badge/REPORT-Technical-f4c2d7?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2512.22334)&#160;
[![GitHub](https://img.shields.io/badge/GitHub-Repository-c7b9e2?style=for-the-badge&logo=github&logoColor=white)](https://github.com/InternScience/SciEvalKit)

<img src="assets/icon/welcome.png" alt="welcome" height="24" style="vertical-align:middle;" />
&nbsp;Welcome to the official repository of <strong>SciEval</strong>!

<div align="center">
  <img src="assets/SciEvalKit.png" alt="SciEval capability radar" width="90%">
</div>

</div>

## <img src="assets/icon/why.png" alt="why" height="28" style="vertical-align:middle;" />&nbsp;Why SciEval?

**SciEval** is an openâ€‘source evaluation framework and leaderboard aimed at measuring the **scientific intelligence** of large language and visionâ€“language models.  
Although modern frontier models often achieve *~90* on generalâ€‘purpose benchmarks, their performance drops sharply on rigorous, domainâ€‘specific scientific tasksâ€”revealing a persistent **generalâ€‘versusâ€‘scientific gap** that motivates the need for SciEval.
Its design is shaped by following core ideas:

- **Beyond generalâ€‘purpose benchmarksâ€‚â–¸** Traditional evaluations focus on surfaceâ€‘level correctness or broadâ€‘domain reasoning, hiding modelsâ€™ weaknesses in realistic scientific problem solving.  SciEval makes this **generalâ€‘versusâ€‘scientific gap** explicit and supplies the evaluation infrastructure needed to guide the integration of broad instructionâ€‘tuned abilities with specialised skills in coding, symbolic reasoning and diagram understanding.
- **Endâ€‘toâ€‘end workflow coverageâ€‚â–¸** SciEval spans the full research pipelineâ€”such as **image interpretation, symbolic reasoning, executable code generation, and hypothesis generation**â€”instead of isolated subtasks.  
- **Capabilityâ€‘oriented & reproducibleâ€‚â–¸** A unified toolkit for **dataset construction, prompt engineering, inference, and expertâ€‘aligned scoring** ensures transparent and repeatable comparisons.  
- **Grounded in real scenariosâ€‚â–¸** Benchmarks use domainâ€‘specific data and tasks so performance reflects **actual scientific practice**, not synthetic proxies.

For a detailed and systematic introduction to SciEvalKit, please refer to the [SciEvalKit Tutorial](https://scievalkit-docs.readthedocs.io/en/latest).


## <img src="assets/icon/progress.png" alt="progress" height="28" style="vertical-align:middle;" />&nbsp;Progress in Scientific Intelligence

*Realtime updates â€” scores are synchronized with the [Internâ€‘Discoveryâ€‘Eval](https://opencompass.org.cn/Intern-Discovery-Eval/rank) leaderboard.*

<div align="center">
  <img src="assets/general_scientific_comparison.png" alt="SciEval capability radar" width="100%">
</div>


- **General benchmarks overestimate scientific competence.** Even the strongest frontier models (e.g., **GeminiÂ 3â€¯Pro**) score below **60** on **Scientific Text Capability** , despite scoring near *90* on widely used generalâ€‘purpose benchmarks.
- **Multimodal capability is breaking the 60â€‘point barrier.** **GeminiÂ 3â€¯Pro** leads **Scientific Multimodal Capability** with **62.88**, reflecting strong performance in multimodal perception and reasoning.
- **Openâ€‘source systems are rapidly closing the gap.** *Qwen3â€‘VLâ€‘235Bâ€‘A22B* and *Qwen3â€‘Max* now match or surpass several proprietary models in symbolic reasoning and code generation, signalling healthy community progress.
- **Symbolic reasoning and code generation remain bottlenecks.** No model exceeds **50** in equationâ€‘level manipulation or **30** in endâ€‘toâ€‘end executable code tasks, indicating that scientific workflows requiring programmatic pipelines still fail frequently.


## <img src="assets/icon/key.png" alt="key" height="28" style="vertical-align:middle;" />&nbsp;Key Features

<div align="center">
  <img src="assets/radar.png" alt="SciEval capability radar" width="70%">
</div>


| Category                                  | Highlights                                                                                                                                                       |
| ----------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Seven Core Dimensions**            | Scientific Knowledge Understanding, Scientific Code Generation, Scientific Symbolic Reasoning, Scientific Hypothesis Generation, Scientific Multimodal Perception, Scientific Multimodal Reasoning, Scientific Multimodal Understanding |
| **Discipline Coverage**             | Life Science â€¢ Astronomy â€¢ Earth Science â€¢ Chemistry â€¢ Materials Science â€¢ Physics.                                                                         |
| **Multimodal & Executable Scoring** | Supports text, code, and image inputs; integrates code tasks and LLM-judge fallback for open-ended answers.                                                      |
| **Reproducible & Extensible**       | Clear dataset and model registries, minimised hard-coding and modular evaluators make new tasks or checkpoints easy to plug in.                                  |

<div align="center">
  <img src="assets/framework.png" alt="SciEval framework overview" width="65%">
</div>

<p align="left">
  <em>
  An overview of the SciEval framework, illustrating how heterogeneous scientific datasets, unified prompt construction, model inference, and capability-oriented evaluators are integrated into a single reproducible evaluation pipeline.
  </em>
</p>


## <img src="assets/icon/news.png" alt="news" height="28" style="vertical-align:middle;" />&nbsp;News
* **[2025â€‘12â€‘12] Â· ğŸ“° Evaluation Published on OpenCompass**
  - SciEvalâ€™s benchmark results are now live on the [OpenCompass](https://opencompass.org.cn/Intern-Discovery-Eval) platform, providing broader community visibility and comparison.

* **[2025â€‘12â€‘05] Â· ğŸš€ SciEvalÂ v1â€¯Launch**
  - Initial public release of a scienceâ€‘focused evaluation toolkit and leaderboard devoted to realistic research workflows.
  - Coverage: seven scientific capability dimensions Ã— six major disciplines in the initial benchmark suite.

* **[2025â€‘12â€‘05] Â· ğŸŒŸ Communityâ€¯Submissionsâ€¯Open**
  - Submit your benchmarks via pull request to appear on the official leaderboard.

## <img src="assets/icon/start.png" alt="start" height="28" style="vertical-align:middle;" />&nbsp;QuickÂ Start

Get from clone to first scores in minutes&mdash;see our local [QuickStart](docs/en/Quickstart.md) / [å¿«é€Ÿå¼€å§‹](docs/zh-CN/Quickstart.md) guides, or refer to the [SciEvalKit Tutorial](https://scievalkit-docs.readthedocs.io/en/latest/Quickstart.html) for additional guidance.

### 1Â Â·Â Install

```bash
git clone https://github.com/InternScience/SciEvalKit.git
cd SciEvalKit
pip install -e .[all]    # brings in vllm, openaiâ€‘sdk, hf_hub, etc.
```

### 2Â Â·Â (Optional) add API keys

Create a `.env` at the repo root **only if** you will call API models or
use an LLMâ€‘asâ€‘judge backend:

```bash
OPENAI_API_KEY=...
GOOGLE_API_KEY=...
DASHSCOPE_API_KEY=...
```

If no keys are provided, SciEval falls back to ruleâ€‘based scoring
whenever possible.

### 3Â Â·Â Run a API demo test

```bash
python run.py \
  --dataset SFE \
  --model gpt-4o \
  --mode all \
  --work-dir outputs/demo_api \
  --verbose
```

### 4Â Â·Â Evaluate a local/GPU model

```bash
python run.py \
  --dataset MaScQA \
  --model qwen_chat \
  --mode infer \
  --work-dir outputs/demo_qwen \
  --verbose

# âœ Reâ€‘run with --mode all after adding an API key
#     if the benchmark requires an LLM judge.
```

## <img src="assets/icon/update.png" alt="update" height="28" style="vertical-align:middle;" />&nbsp;CodebaseÂ Updates

* **Executionâ€‘basedÂ Scoring**
  - Codeâ€‘generation tasks (SciCode,Â AstroVisBench) are now graded via sandboxed unit tests.

---

## ğŸ“¬ Contact Us

- ğŸ’¬ **GitHub Issues**: Please open an issue for bug reports or feature requests

- ğŸ¤ **Community**: 

<p align="center">
  <img src="https://raw.githubusercontent.com/InternScience/SGI-Bench/main/assets/wechat.jpg" alt="WeChat" width="200">
</p>

---

## <img src="assets/icon/thanks.png" alt="thanks" height="30" style="vertical-align:middle;" />&nbsp;Acknowledgements

SciEval ToolKit is built on top of the excellent **[VLMEvalKit](https://github.com/open-compass/VLMEvalKit)** framework andâ€¯we thank the OpenCompass team not only for openâ€‘sourcing their engine, but also for publishing thorough deployment and development guides ([Quickâ€¯Start](https://vlmevalkit.readthedocs.io/en/latest/Quickstart.html),Â [Development Notes](https://vlmevalkit.readthedocs.io/en/latest/Development.html)) that streamlined our integration.

We also acknowledge the core SciEval contributors for their efforts on dataset curation, evaluation design, and engine implementation: JunÂ Yao, HanÂ Deng, YizhouÂ Wang, JiabeiÂ Xiao, JiaqiÂ Liu, EnchengÂ Su, YujieÂ Liu, WeidaÂ Wang, JunchiÂ Yao, HaoranÂ Sun, RunminÂ Ma, BoÂ Zhang, DongzhanÂ Zhou, ShufeiÂ Zhang, PengÂ Ye, XiaosongÂ Wang, and ShixiangÂ Tang, as well as all community testers who provided early feedback.

SciEvalKit contributors can join the author list of the report based on their contribution to the repository. Specifically, it requires 3 major contributions (implement a new benchmark, foundation model, or contribute a major feature). We will update the report quarterly and an additional section that details each developerâ€™s contribution will be appended in the next update.
